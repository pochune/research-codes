{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-26T09:00:24.031010Z",
     "iopub.status.busy": "2023-04-26T09:00:24.029926Z",
     "iopub.status.idle": "2023-04-26T09:00:24.039460Z",
     "shell.execute_reply": "2023-04-26T09:00:24.038336Z",
     "shell.execute_reply.started": "2023-04-26T09:00:24.030970Z"
    },
    "id": "hz62_mbWWhgh"
   },
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import math\n",
    "\n",
    "# Data handling and numerical processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Machine Learning and Deep Learning\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
    "\n",
    "# Keras and TensorFlow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# Plotting and Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "execution": {
     "iopub.execute_input": "2023-04-26T09:00:28.222222Z",
     "iopub.status.busy": "2023-04-26T09:00:28.221870Z",
     "iopub.status.idle": "2023-04-26T09:00:28.253307Z",
     "shell.execute_reply": "2023-04-26T09:00:28.252405Z",
     "shell.execute_reply.started": "2023-04-26T09:00:28.222186Z"
    },
    "id": "lYhzOwz5Whgs",
    "outputId": "2394fe4a-2bcb-42de-bc69-86aa5874ba5c"
   },
   "outputs": [],
   "source": [
    "df_origin=pd.read_csv('Original file location')\n",
    "df_main = df_origin[1000:37000]\n",
    "scaler = MinMaxScaler(feature_range = (0,1))\n",
    "df_scaled = scaler.fit_transform(df_main)\n",
    "df_mm = pd.DataFrame(df_scaled, columns = ['time', 'slip rate', 'accumlated slip', 'shortening', 'normal stress', 'shear stree', 'friction', 'temperature', 'short av', 'normal av', 'shear av', 'friction av', 'temp av', 'slip av', 'energy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_df = df_mm[['inp_parameter_label','friction av']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "execution": {
     "iopub.execute_input": "2023-04-26T09:00:37.978871Z",
     "iopub.status.busy": "2023-04-26T09:00:37.978270Z",
     "iopub.status.idle": "2023-04-26T09:00:37.998825Z",
     "shell.execute_reply": "2023-04-26T09:00:37.997630Z",
     "shell.execute_reply.started": "2023-04-26T09:00:37.978821Z"
    },
    "id": "sfj7o7NFWhg7",
    "outputId": "762eafb6-103e-408a-cf1e-e8c8b4a19ac6"
   },
   "outputs": [],
   "source": [
    "df_inp_origin = scaled_df\n",
    "df_inp_n = df_inp_origin*1000\n",
    "df_inp = df_inp_n.astype(int)\n",
    "df_inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-04-26T09:00:39.273870Z",
     "iopub.status.busy": "2023-04-26T09:00:39.272976Z",
     "iopub.status.idle": "2023-04-26T09:00:39.285258Z",
     "shell.execute_reply": "2023-04-26T09:00:39.284081Z",
     "shell.execute_reply.started": "2023-04-26T09:00:39.273829Z"
    },
    "id": "qZaC9DfGWhg8",
    "outputId": "9844cc00-439a-448a-976b-17d81e60aba6"
   },
   "outputs": [],
   "source": [
    "data = df_inp\n",
    "dataset = data.values\n",
    "training_data_len = math.ceil(len(dataset)*0.8)\n",
    "training_data = dataset[0:training_data_len, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_train_time = 10\n",
    "set_pred_time = 5\n",
    "x_train_or = []\n",
    "y_train = []\n",
    "\n",
    "for i in range((set_train_time + set_pred_time), training_data.shape[0]):\n",
    "    x_train_or.append(training_data[i-(set_train_time + set_pred_time):i-(set_pred_time), 0:-1])\n",
    "    y_train.append(training_data[i-(set_train_time-1):i+1, -1])\n",
    "\n",
    "x_train_or = np.array(x_train_or)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "y_train_expanded = np.expand_dims(y_train, axis=-1)\n",
    "train_data = np.concatenate((x_train_or, y_train_expanded), axis=-1)\n",
    "train_data = train_data.astype(np.int64)\n",
    "\n",
    "print(train_data.shape)  # Should print (5040, 50, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-04-26T09:00:39.519491Z",
     "iopub.status.busy": "2023-04-26T09:00:39.519010Z",
     "iopub.status.idle": "2023-04-26T09:00:42.810961Z",
     "shell.execute_reply": "2023-04-26T09:00:42.809868Z",
     "shell.execute_reply.started": "2023-04-26T09:00:39.519445Z"
    },
    "id": "bEuD350lWhg9",
    "outputId": "b3ef495c-86df-41f4-d863-33a6c63d9433"
   },
   "outputs": [],
   "source": [
    "def tensor_change(x, y):\n",
    "    x = tf.convert_to_tensor(x, dtype=tf.int64)\n",
    "    y = tf.convert_to_tensor(y, dtype=tf.int64)\n",
    "    return x, y\n",
    "\n",
    "BUFFER_SIZE = 1000\n",
    "BATCH_SIZE = 256\n",
    "train_time = 5\n",
    "pred_time = 5\n",
    "\n",
    "def make_batches_train(train_data):\n",
    "    x_data = train_data[:, 4:(train_time+5), 0] \n",
    "    y_data = train_data[:, (pred_time-2):-1, -1]\n",
    "\n",
    "    ds = tf.data.Dataset.from_tensor_slices((x_data, y_data))\n",
    "\n",
    "    return (\n",
    "        ds\n",
    "        .cache()\n",
    "        .shuffle(BUFFER_SIZE)\n",
    "        .batch(BATCH_SIZE)\n",
    "        .map(tensor_change, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        .prefetch(tf.data.AUTOTUNE))\n",
    "\n",
    "train_batches = make_batches_train(train_data)\n",
    "x_train_batch, y_train_batch = next(iter(train_batches))\n",
    "x_train_batch, y_train_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-04-26T09:00:42.978846Z",
     "iopub.status.busy": "2023-04-26T09:00:42.978414Z",
     "iopub.status.idle": "2023-04-26T09:00:43.008824Z",
     "shell.execute_reply": "2023-04-26T09:00:43.007772Z",
     "shell.execute_reply.started": "2023-04-26T09:00:42.978802Z"
    },
    "id": "CVv1K5BWWhg-",
    "outputId": "2d150e49-0ceb-40b7-bbe7-15b08abd7346"
   },
   "outputs": [],
   "source": [
    "val_data = dataset[training_data_len - (train_time):, :]\n",
    "\n",
    "x_val_or = []\n",
    "y_val = []\n",
    "\n",
    "for i in range((train_time + pred_time), val_data.shape[0]):\n",
    "    x_val_or.append(val_data[i-(train_time + pred_time):i-(pred_time), 0:-1])\n",
    "    y_val.append(val_data[i-(train_time-1):i+1, -1])\n",
    "\n",
    "x_val_or = np.array(x_val_or)\n",
    "y_val = np.array(y_val)\n",
    "\n",
    "y_val_expanded = np.expand_dims(y_val, axis=-1)\n",
    "\n",
    "val_data = np.concatenate((x_val_or, y_val_expanded), axis=-1)\n",
    "\n",
    "val_data = val_data.astype(np.int64)\n",
    "\n",
    "print(val_data.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FfvRTJPWtN6S"
   },
   "outputs": [],
   "source": [
    "def make_batches_val(val_data):\n",
    "    x_data = train_data[:, 4:(train_time+5), 0]\n",
    "    y_data = train_data[:, (pred_time-2):-1, -1]\n",
    "\n",
    "    ds = tf.data.Dataset.from_tensor_slices((x_data, y_data))\n",
    "\n",
    "    return (\n",
    "        ds\n",
    "        .cache()\n",
    "        .shuffle(BUFFER_SIZE)\n",
    "        .batch(BATCH_SIZE)\n",
    "        .map(tensor_change, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        .prefetch(tf.data.AUTOTUNE))\n",
    "\n",
    "val_batches = make_batches_val(val_data)\n",
    "x_val_batch, y_val_batch = next(iter(val_batches))\n",
    "x_val_batch, y_val_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build Transformer Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-26T09:00:43.092064Z",
     "iopub.status.busy": "2023-04-26T09:00:43.090383Z",
     "iopub.status.idle": "2023-04-26T09:00:43.103907Z",
     "shell.execute_reply": "2023-04-26T09:00:43.102901Z",
     "shell.execute_reply.started": "2023-04-26T09:00:43.092024Z"
    },
    "id": "WJoNW90pWhhA"
   },
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "    return pos * angle_rates\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :], d_model)\n",
    "\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "def tar_create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask\n",
    "\n",
    "def tar_create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def inp_create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask\n",
    "\n",
    "def inp_create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "    output = tf.matmul(attention_weights, v)\n",
    "    return output, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-26T09:00:43.139913Z",
     "iopub.status.busy": "2023-04-26T09:00:43.138731Z",
     "iopub.status.idle": "2023-04-26T09:00:43.159484Z",
     "shell.execute_reply": "2023-04-26T09:00:43.158208Z",
     "shell.execute_reply.started": "2023-04-26T09:00:43.139877Z"
    },
    "id": "-vLc_0DHWhhC"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        # Linear Layer\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "\n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)\n",
    "        k = self.wk(k)\n",
    "        v = self.wv(v)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "\n",
    "        concat_attention = tf.reshape(scaled_attention,\n",
    "                          (batch_size, -1, self.d_model))\n",
    "\n",
    "        output = self.dense(concat_attention)\n",
    "\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-26T09:00:43.168830Z",
     "iopub.status.busy": "2023-04-26T09:00:43.166420Z",
     "iopub.status.idle": "2023-04-26T09:00:43.175730Z",
     "shell.execute_reply": "2023-04-26T09:00:43.174744Z",
     "shell.execute_reply.started": "2023-04-26T09:00:43.168791Z"
    },
    "id": "FmCsRdZTWhhC"
   },
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(dff, activation='relu'),\n",
    "        tf.keras.layers.Dense(d_model)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-26T09:00:43.183083Z",
     "iopub.status.busy": "2023-04-26T09:00:43.180484Z",
     "iopub.status.idle": "2023-04-26T09:00:43.194429Z",
     "shell.execute_reply": "2023-04-26T09:00:43.193335Z",
     "shell.execute_reply.started": "2023-04-26T09:00:43.183045Z"
    },
    "id": "N6ixzKgJWhhD"
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "\n",
    "        attn_output, _ = self.mha(x, x, x, mask)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)\n",
    "\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-26T09:00:43.202240Z",
     "iopub.status.busy": "2023-04-26T09:00:43.199607Z",
     "iopub.status.idle": "2023-04-26T09:00:43.216214Z",
     "shell.execute_reply": "2023-04-26T09:00:43.215166Z",
     "shell.execute_reply.started": "2023-04-26T09:00:43.202201Z"
    },
    "id": "UHCK3ECMWhhE"
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "\n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "\n",
    "        attn1, attn_weights_block1 \\\n",
    "                = self.mha1(x, x, x, look_ahead_mask)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "        attn2, attn_weights_block2 \\\n",
    "                = self.mha2(enc_output, enc_output, out1, padding_mask)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)\n",
    "\n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)\n",
    "\n",
    "        return out3, attn_weights_block1, attn_weights_block2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-26T09:00:43.223833Z",
     "iopub.status.busy": "2023-04-26T09:00:43.221038Z",
     "iopub.status.idle": "2023-04-26T09:00:43.236300Z",
     "shell.execute_reply": "2023-04-26T09:00:43.235170Z",
     "shell.execute_reply.started": "2023-04-26T09:00:43.223730Z"
    },
    "id": "JW3LF5gFWhhE"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-26T09:00:43.243900Z",
     "iopub.status.busy": "2023-04-26T09:00:43.241424Z",
     "iopub.status.idle": "2023-04-26T09:00:43.258479Z",
     "shell.execute_reply": "2023-04-26T09:00:43.257422Z",
     "shell.execute_reply.started": "2023-04-26T09:00:43.243861Z"
    },
    "id": "bKIjKLISWhhF"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "\n",
    "        self.dec_layers \\\n",
    "             = [DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "\n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "          x, block1, block2 \\\n",
    "            = self.dec_layers[i](x, enc_output, training, look_ahead_mask, padding_mask)\n",
    "\n",
    "          attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
    "          attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
    "\n",
    "        return x, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-26T09:00:43.265968Z",
     "iopub.status.busy": "2023-04-26T09:00:43.263328Z",
     "iopub.status.idle": "2023-04-26T09:00:43.280908Z",
     "shell.execute_reply": "2023-04-26T09:00:43.279908Z",
     "shell.execute_reply.started": "2023-04-26T09:00:43.265926Z"
    },
    "id": "bUQfo0GNWhhF"
   },
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model,  num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, dff,input_vocab_size, pe_input, rate)\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate)\n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size) \n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        inp, tar = inputs\n",
    "        enc_padding_mask, look_ahead_mask, dec_padding_mask = self.create_masks(inp, tar)\n",
    "        enc_output = self.encoder(inp, training, enc_padding_mask) \n",
    "        dec_output, attention_weights = self.decoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "        final_output = self.final_layer(dec_output)\n",
    "        return final_output, attention_weights\n",
    "\n",
    "    def create_masks(self, inp, tar):\n",
    "        enc_padding_mask = inp_create_padding_mask(inp)\n",
    "        dec_padding_mask = inp_create_padding_mask(inp)\n",
    "        look_ahead_mask = tar_create_look_ahead_mask(tf.shape(tar)[1])\n",
    "        dec_target_padding_mask = tar_create_padding_mask(tar)\n",
    "        look_ahead_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "\n",
    "        return enc_padding_mask, look_ahead_mask, dec_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-26T09:00:43.288301Z",
     "iopub.status.busy": "2023-04-26T09:00:43.285742Z",
     "iopub.status.idle": "2023-04-26T09:00:43.294934Z",
     "shell.execute_reply": "2023-04-26T09:00:43.293858Z",
     "shell.execute_reply.started": "2023-04-26T09:00:43.288251Z"
    },
    "id": "VzFwnWlOWhhG"
   },
   "outputs": [],
   "source": [
    "num_layers =   #ref = 6\n",
    "d_model =      #ref = 256 or 512\n",
    "dff =          #ref = 1024 or 2048\n",
    "num_heads =    #ref = 6 to 8\n",
    "dropout_rate = #ref = 0.1 to 0.2\n",
    "input_vocab_size = #ref = 1001\n",
    "target_vocab_size = #ref = 1001\n",
    "pe_input =     #ref = 1 to 6\n",
    "pe_target =    #ref = 1to 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set training conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-26T09:00:43.470624Z",
     "iopub.status.busy": "2023-04-26T09:00:43.469531Z",
     "iopub.status.idle": "2023-04-26T09:00:43.480515Z",
     "shell.execute_reply": "2023-04-26T09:00:43.479089Z",
     "shell.execute_reply.started": "2023-04-26T09:00:43.470577Z"
    },
    "id": "SlwQ3zRYWhhG"
   },
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=6000):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, dtype=tf.float32)\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-26T09:00:43.694848Z",
     "iopub.status.busy": "2023-04-26T09:00:43.694244Z",
     "iopub.status.idle": "2023-04-26T09:00:43.738554Z",
     "shell.execute_reply": "2023-04-26T09:00:43.737429Z",
     "shell.execute_reply.started": "2023-04-26T09:00:43.694798Z"
    },
    "id": "_KqxRaZEWhhH"
   },
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9, clipvalue=0.5)\n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-26T09:00:45.123854Z",
     "iopub.status.busy": "2023-04-26T09:00:45.123138Z",
     "iopub.status.idle": "2023-04-26T09:00:45.131246Z",
     "shell.execute_reply": "2023-04-26T09:00:45.130178Z",
     "shell.execute_reply.started": "2023-04-26T09:00:45.123812Z"
    },
    "id": "tNZIKHQJWhhI"
   },
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "\n",
    "  return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
    "\n",
    "def accuracy_function(real, pred):\n",
    "  accuracies = tf.equal(real, tf.argmax(pred, axis=2))\n",
    "\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  accuracies = tf.math.logical_and(mask, accuracies)\n",
    "\n",
    "  accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
    "  mask = tf.cast(mask, dtype=tf.float32)\n",
    "  return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZGFVtubAUNeC"
   },
   "outputs": [],
   "source": [
    "def loss_function_val(real, pred):\n",
    "    mse = tf.keras.losses.MeanSquaredError()\n",
    "    loss = mse(real, pred)\n",
    "    return  tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R3SV2nrLtN6b"
   },
   "outputs": [],
   "source": [
    "def accuracy_function_val(real, pred):\n",
    "    mape = mean_absolute_percentage_error\n",
    "    loss = mape(real, pred)\n",
    "    return  tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-26T09:00:45.860910Z",
     "iopub.status.busy": "2023-04-26T09:00:45.860223Z",
     "iopub.status.idle": "2023-04-26T09:00:45.878751Z",
     "shell.execute_reply": "2023-04-26T09:00:45.877727Z",
     "shell.execute_reply.started": "2023-04-26T09:00:45.860870Z"
    },
    "id": "J4zICy1wWhhJ"
   },
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-26T09:00:46.433197Z",
     "iopub.status.busy": "2023-04-26T09:00:46.432828Z",
     "iopub.status.idle": "2023-04-26T09:00:46.580231Z",
     "shell.execute_reply": "2023-04-26T09:00:46.579245Z",
     "shell.execute_reply.started": "2023-04-26T09:00:46.433163Z"
    },
    "id": "weusffECWhhJ"
   },
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    dff=dff,\n",
    "    input_vocab_size=input_vocab_size, \n",
    "    target_vocab_size=input_vocab_size,\n",
    "    pe_input=pe_input, \n",
    "    pe_target=pe_target,\n",
    "    rate=dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-26T09:00:47.059928Z",
     "iopub.status.busy": "2023-04-26T09:00:47.058633Z",
     "iopub.status.idle": "2023-04-26T09:00:47.068693Z",
     "shell.execute_reply": "2023-04-26T09:00:47.066996Z",
     "shell.execute_reply.started": "2023-04-26T09:00:47.059875Z"
    },
    "id": "v8Pi-xKNWhhL"
   },
   "outputs": [],
   "source": [
    "checkpoint_path = \"./checkpoints/train\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "                           optimizer=optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-26T09:00:48.638079Z",
     "iopub.status.busy": "2023-04-26T09:00:48.637716Z",
     "iopub.status.idle": "2023-04-26T09:00:48.646463Z",
     "shell.execute_reply": "2023-04-26T09:00:48.645429Z",
     "shell.execute_reply.started": "2023-04-26T09:00:48.638046Z"
    },
    "id": "9bcMRTHKWhhM"
   },
   "outputs": [],
   "source": [
    "train_step_signature = [\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int64), ##################################num_feature지정\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "]\n",
    "\n",
    "\n",
    "@tf.function(input_signature=train_step_signature)\n",
    "\n",
    "def train_step(inp, tar):\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, _ = transformer([inp, tar_inp], training = True)\n",
    "        loss = loss_function(tar_real, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(accuracy_function(tar_real, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TNqEh0OktN6d"
   },
   "outputs": [],
   "source": [
    "def diff(tensor, axis=-1):\n",
    "    return tensor[..., 1:] - tensor[..., :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Translator(tf.Module):\n",
    "    def __init__(self, transformer):\n",
    "        self.transformer = transformer\n",
    "\n",
    "    def __call__(self, inp_single, tar_single, max_length=None):\n",
    "        encoder_input = inp_single[:, :] \n",
    "        start = tf.expand_dims(tar_single[0,0], axis=0)\n",
    "\n",
    "        output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n",
    "        output_array = output_array.write(0, start)\n",
    "\n",
    "        for i in tf.range(max_length):\n",
    "            output = tf.transpose(output_array.stack())\n",
    "            predictions, _ = self.transformer([encoder_input, output], training=False)\n",
    "            predictions = predictions[:, -1:, :]\n",
    "            predicted_id = tf.argmax(predictions, axis=-1)\n",
    "            output_array = output_array.write(i + 1, predicted_id[0])\n",
    "\n",
    "        output = tf.transpose(output_array.stack())\n",
    "        output = output[:, 1:] \n",
    "\n",
    "        _, attention_weights = self.transformer([encoder_input, output[:, :-1]], training=False)\n",
    "\n",
    "        return output, attention_weights\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_validation_batches(translator, val_batches, num_val_batches=None, max_length=None):\n",
    "\n",
    "    val_loss_value = tf.keras.metrics.Mean(name='val_loss_value')\n",
    "    val_loss_delta = tf.keras.metrics.Mean(name='val_loss_delta')\n",
    "    val_accuracy = tf.keras.metrics.Mean(name='val_accuracy')\n",
    "    \n",
    "    for (batch, (inp, tar)) in enumerate(val_batches.take(num_val_batches)):\n",
    "        batch_size = inp.shape[0]\n",
    "        predictions_list = []\n",
    "        tar_total_list = []\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            inp_single = inp[i:i+1]  \n",
    "            tar_single = tar[i:i+1]\n",
    "            outputs, _ = translator(inp_single, tar_single, max_length)\n",
    "            predictions_list.append(outputs)\n",
    "\n",
    "            tar_single = tar[i:i+1]  \n",
    "            tar_total_list.append(tar_single)\n",
    "\n",
    "        predictions = tf.concat(predictions_list, axis=0)\n",
    "        print(f'prediction value {predictions.numpy()}') \n",
    "        tar_total = tf.concat(tar_total_list, axis = 0)\n",
    "        print(f'true value {tar_total.numpy()}')\n",
    "\n",
    "        delta_predictions = diff(predictions, axis=1)\n",
    "        delta_tar_total = diff(tar_total, axis=1)\n",
    "\n",
    "        loss_val = loss_function_val(tar_total, predictions)\n",
    "        loss_del = loss_function_val(delta_tar_total, delta_predictions)\n",
    "\n",
    "        validation1 = val_loss_value(loss_val)\n",
    "        validation2 = val_loss_delta(loss_del)\n",
    "\n",
    "        print(f'val_loss_value {validation1}')\n",
    "        print(f'val_loss_delta {validation2}')\n",
    "    return val_loss_value.result(), val_loss_delta.result()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "execution": {
     "iopub.execute_input": "2023-04-26T09:04:22.188257Z",
     "iopub.status.busy": "2023-04-26T09:04:22.187898Z",
     "iopub.status.idle": "2023-04-26T09:20:07.054233Z",
     "shell.execute_reply": "2023-04-26T09:20:07.052784Z",
     "shell.execute_reply.started": "2023-04-26T09:04:22.188225Z"
    },
    "id": "VvQ8QaCgWhhN",
    "outputId": "92ced035-3d41-44b3-bce4-662f4a3fca11"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 200\n",
    "num_val_batches = # ref = number of validation dataset/batch size\n",
    "max_length = #ref = 6\n",
    "translator = Translator(transformer)\n",
    "import time\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "\n",
    "    for (batch, (inp, tar)) in enumerate(train_batches):\n",
    "        train_step(inp, tar)\n",
    "\n",
    "        if batch % 50 == 0:\n",
    "            print(f'Epoch {epoch + 1} Batch {batch} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
    "\n",
    "    val_loss_value, val_loss_delta = evaluate_validation_batches(translator, val_batches, num_val_batches, max_length)\n",
    "    print(f'Validation Loss Value: {val_loss_value:.4f}')\n",
    "    print(f'Validation Loss Delta: {val_loss_delta:.4f}')\n",
    "\n",
    "    ckpt_save_path = ckpt_manager.save()\n",
    "    print(f'Saving checkpoint for epoch {epoch+1} at {ckpt_save_path}')\n",
    "\n",
    "    print(f'Epoch {epoch + 1} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
    "\n",
    "    print(f'Time taken for 1 epoch: {time.time() - start:.2f} secs\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load checkpoint model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mNwyYlmjtN6g",
    "outputId": "86548686-d4c3-488a-bc59-2405eef24b50"
   },
   "outputs": [],
   "source": [
    "checkpoint_path_specific = \"./checkpoints/train/ckpt-26\"  \n",
    "\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "                           optimizer=optimizer)\n",
    "\n",
    "\n",
    "ckpt.restore(checkpoint_path_specific)\n",
    "print('Specific checkpoint restored!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test loaded model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N6q659sptN6g"
   },
   "outputs": [],
   "source": [
    "def make_batches_sequential(data):\n",
    "    x_data = data[:, 4:(train_time+5), 0]\n",
    "    y_data = data[:, (pred_time-2):-1, -1]\n",
    "    ds = tf.data.Dataset.from_tensor_slices((x_data, y_data))\n",
    "\n",
    "    return (\n",
    "        ds\n",
    "        .cache()\n",
    "        .batch(BATCH_SIZE)\n",
    "        .map(tensor_change, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        .prefetch(tf.data.AUTOTUNE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hoW3WudGtN6h"
   },
   "outputs": [],
   "source": [
    "val_batches_seq = make_batches_sequential(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AcSvmk5jtN6h",
    "outputId": "818c6305-4fcf-474a-e27c-6a5fdb6aad1c"
   },
   "outputs": [],
   "source": [
    "translator = Translator(transformer)\n",
    "def predict_batches(translator, val_batches, max_length=None):\n",
    "    predictions_list = []\n",
    "\n",
    "    for (batch, (inp, tar)) in enumerate(val_batches_seq):\n",
    "        batch_size = inp.shape[0]\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            inp_single = inp[i:i+1]\n",
    "            tar_single = tar[i:i+1]\n",
    "            outputs, _ = translator(inp_single, tar_single, max_length)\n",
    "            predictions_list.append(outputs)\n",
    "\n",
    "    predictions = tf.concat(predictions_list, axis=0)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "predictions = predict_batches(translator, val_batches_seq, max_length=6)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#estimate attention weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4fC4aoE3tN6l"
   },
   "outputs": [],
   "source": [
    "class Trans(tf.Module):\n",
    "    def __init__(self, transformer):\n",
    "        self.transformer = transformer\n",
    "\n",
    "    def __call__(self, inp_single, max_length=None):\n",
    "        encoder_input = inp_single[:,:]\n",
    "\n",
    "        start = tf.expand_dims(inp_single[0, -1], axis=0)\n",
    "\n",
    "        output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n",
    "        output_array = output_array.write(0, start)\n",
    "\n",
    "        for i in tf.range(max_length):\n",
    "            output = tf.transpose(output_array.stack())\n",
    "            predictions, _ = self.transformer([encoder_input, output], training=False)\n",
    "\n",
    "            predictions = predictions[:, -1:, :]\n",
    "\n",
    "            predicted_id = tf.argmax(predictions, axis=-1)\n",
    "\n",
    "\n",
    "            output_array = output_array.write(i + 1, predicted_id[0])\n",
    "\n",
    "        output = tf.transpose(output_array.stack())\n",
    "        output = output[:, 1:] \n",
    "\n",
    "        _, attention_weights = self.transformer([encoder_input, output[:, :-1]], training=False)\n",
    "\n",
    "        return output, attention_weights\n",
    "\n",
    "def translator1(trans, val_batches, max_length=None):\n",
    "    inp_single = val_batches\n",
    "    outputs, attention_weight = trans(inp_single, max_length)\n",
    "    return outputs, attention_weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GQS8ky1YtN6l",
    "outputId": "23344175-8d04-407f-cfc1-0d38bcaa691e"
   },
   "outputs": [],
   "source": [
    "loc1= #ref = data index - 1\n",
    "loc2= #ref = data index\n",
    "trans = Trans(transformer)\n",
    "im_input=x_val_seq_batch[loc1:loc2] \n",
    "val_data1=y_val_seq_batch[loc1:loc2]\n",
    "print(val_data1)\n",
    "print(im_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VZIOrT24tN6m",
    "outputId": "5c4acbd6-c576-441a-c84f-94c6c14d55a7"
   },
   "outputs": [],
   "source": [
    "output, attention_weights = translator1(trans, im_input, max_length = 6)\n",
    "print(output)\n",
    "print(attention_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z39W8pjVtN6m",
    "outputId": "6a55dcc5-c201-4ec6-f838-138dd1f70036"
   },
   "outputs": [],
   "source": [
    "head = 0\n",
    "attention_heads = tf.squeeze(attention_weights['decoder_layer6_block2'], 0)\n",
    "attention = attention_heads[head]\n",
    "attention.shape\n",
    "print(attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention_head(im_input, output, attention, filename=\"plot.png\"):\n",
    "    # convert tensor to numpy\n",
    "    im_input = im_input.numpy()\n",
    "    output = output.numpy()\n",
    "\n",
    "    # Normalize attention values to range [0, 10]\n",
    "    attention = 10 * (attention - np.min(attention)) / (np.max(attention) - np.min(attention))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8,8))\n",
    "    cax = ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "    # Define the labels for the ticks\n",
    "    x_labels = [1, 2, 3, 4, 5, 6]\n",
    "    y_labels = [1, 2, 3, 4, 5]\n",
    "\n",
    "    ax.set_xticks(range(len(x_labels)))\n",
    "    ax.set_yticks(range(len(y_labels)))\n",
    "\n",
    "    ax.set_xticklabels(x_labels)\n",
    "    ax.set_yticklabels(y_labels)\n",
    "\n",
    "    # Add the attention values as text annotations\n",
    "    for i in range(attention.shape[0]):\n",
    "        for j in range(attention.shape[1]):\n",
    "            text = ax.text(j, i, f'{attention[i,j]:.2f}',\n",
    "                       ha=\"center\", va=\"center\", color=\"w\")\n",
    "\n",
    "    # Add a colorbar legend\n",
    "    cbar_ax = fig.add_axes([0.92, 0.15, 0.03, 0.7])  # Adjust these parameters to change the position and size of the colorbar\n",
    "    plt.colorbar(cax, cax=cbar_ax)\n",
    "\n",
    "    # Save the figure as a high-resolution PNG\n",
    "    plt.savefig(filename, dpi=300)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aOlccFrutN6n",
    "outputId": "a6948aea-f405-4b26-f50b-2f11580bcb01"
   },
   "outputs": [],
   "source": [
    "plot_attention_head(im_input, output, attention, filename=\"imp921.png\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
